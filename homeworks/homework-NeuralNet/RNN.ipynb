{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is a Simple Recurrent Network'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "### Load Data ###\n",
    "def sentenceToTensor(tokens_list):\n",
    "    # Convert list of strings to tensor of token indices (integers)\n",
    "    #\n",
    "    # Input\n",
    "    #  tokens_list : list of strings, e.g. ['<SOS>','lion','eat','man','<EOS>']\n",
    "    # Output\n",
    "    #  1D tensor of the same length (integers), e.g., tensor([ 2, 18, 13, 19,  0])\n",
    "    assert(isinstance(tokens_list,list))\n",
    "    tokens_index = [token_to_index[token] for token in tokens_list]\n",
    "    return torch.tensor(tokens_index)\n",
    "\n",
    "# load and process the set of simple sentences\n",
    "with open('data/elman_sentences.txt','r') as fid:\n",
    "    lines = fid.readlines()\n",
    "sentences_str = [l.strip() for l in lines]\n",
    "sentences_tokens = [s.split() for s in sentences_str]\n",
    "sentences_tokens = [['<SOS>']+s+['<EOS>'] for s in sentences_tokens]\n",
    "unique_tokens = sorted(set(sum(sentences_tokens,[])))\n",
    "n_tokens = len(unique_tokens) # all words and special tokens\n",
    "token_to_index = {t : i for i,t in enumerate(unique_tokens)}\n",
    "index_to_token = {i : t for i,t in enumerate(unique_tokens)}\n",
    "training_pats = [sentenceToTensor(s) for s in sentences_tokens] # python list of 1D sentence tensors\n",
    "ntrain = len(training_pats)\n",
    "print('mapping unique tokens to integers: %s \\n' % token_to_index)\n",
    "print('example sentence as string: %s \\n' % ' '.join(sentences_tokens[0]))\n",
    "print('example sentence as tensor: %s \\n' % training_pats[0])\n",
    "\n",
    "### Build Neural Network ###\n",
    "# Word level SRN, only take single word at a time\n",
    "class SRN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        # vocab_size : number of tokens in vocabulary including special tokens <SOS> and <EOS>\n",
    "        # hidden_size : dim of input embeddings and hidden layer\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size,hidden_size)\n",
    "        self.fc1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_token_index, hidden_prev):\n",
    "        # Input\n",
    "        #    input_token_index: [integer] index of current input token\n",
    "        #    hidden_prev: [length hidden_size 1D tensor] hidden state from previous step\n",
    "        # Output\n",
    "        #    output: [length vocab_size 1D tensor] log-probability of emitting each output token\n",
    "        #    hidden_curr : [length hidden_size 1D tensor] hidden state for current step\n",
    "        input_embed = self.embed(input_token_index) # hidden_size 1D tensor\n",
    "        hidden_curr = nn.functional.sigmoid(self.fc1(torch.cat((input_embed, hidden_prev))))\n",
    "        output = nn.functional.log_softmax(self.fc2(hidden_curr))\n",
    "        return output, hidden_curr\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Returns length hidden_size 1D tensor of zeros\n",
    "        return torch.zeros(self.hidden_size)\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        # Returns [vocab_size x hidden_size] numpy array of input embeddings\n",
    "        return self.embed(torch.arange(self.vocab_size)).detach().numpy()\n",
    "    \n",
    "### Train Neural Network ###\n",
    "def train(seq_tensor, rnn):\n",
    "    # Process a sentence and update the SRN weights. With <SOS> as the input at step 0,\n",
    "    # predict every subsequent word given the past words.\n",
    "    # Return the mean loss across each symbol prediction.\n",
    "    #\n",
    "    # Input\n",
    "    #   seq_tensor: [1D tensor] sentence as token indices\n",
    "    #   rnn : instance of SRN class\n",
    "    # Output\n",
    "    #   loss : [scalar] average NLL loss across prediction steps\n",
    "    rnn.train()\n",
    "    hidden_prev = rnn.initHidden()\n",
    "    output = torch.zeros(len(seq_tensor)-1, rnn.vocab_size)\n",
    "    rnn.zero_grad()\n",
    "    for i in range(len(seq_tensor)-1):\n",
    "        output[i], hidden_prev = rnn(seq_tensor[i], hidden_prev)\n",
    "        \n",
    "    loss = criterion(output, seq_tensor[1:])\n",
    "    return loss\n",
    "\n",
    "# Main training loop\n",
    "nepochs = 10 # number of passes through the entire training set \n",
    "nhidden = 20 # number of hidden units in the SRN\n",
    "rnn = SRN(n_tokens,nhidden)\n",
    "optimizer = torch.optim.AdamW(rnn.parameters(), weight_decay=0.04) # w/ default learning rate 0.001\n",
    "criterion = nn.NLLLoss()\n",
    "n = len(training_pats)\n",
    "for i in range(nepochs):\n",
    "    perm = np.random.permutation(n)\n",
    "    train_error = 0\n",
    "    for p in perm:\n",
    "        loss = train(training_pats[p], rnn)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_error += loss.item()\n",
    "    print(train_error/float(n))\n",
    "    \n",
    "### Analyze the SRN Internal Representation ###\n",
    "def plot_dendo(X, names, exclude=['<SOS>','<EOS>']):\n",
    "    #  Show hierarchical clustering of vectors \n",
    "    #\n",
    "    # Input\n",
    "    #  X : numpy tensor [nitem x dim] such that each row is a vector to be clustered\n",
    "    #  names : [length nitem] list of item names\n",
    "    #  exclude: list of names we want to exclude       \n",
    "    nitem = len(names)\n",
    "    names  = np.array(names)\n",
    "    include = np.array([myname not in exclude for myname in names], dtype=bool)\n",
    "    linked = linkage(X[include],'single', optimal_ordering=True)\n",
    "    plt.figure(1, figsize=(20,6))\n",
    "    dendrogram(linked, labels=names[include], color_threshold=0, leaf_font_size=18)\n",
    "    plt.show()\n",
    "\n",
    "plot_dendo(rnn.get_embeddings(), unique_tokens)\n",
    "\n",
    "### Generate Sentences ###\n",
    "def generate(rnn, maxlen=4):\n",
    "    hidden_prev = rnn.initHidden()\n",
    "    sentence = []\n",
    "    word_index = torch.tensor(1)\n",
    "    for i in range(maxlen):\n",
    "        output_tens, hidden_prev = rnn.forward(word_index, hidden_prev)\n",
    "        cat = torch.distributions.categorical.Categorical(output_tens)\n",
    "        word_index = cat.sample()\n",
    "        sentence.append(index_to_token[word_index.item()])\n",
    "    return print(' '.join(sentence))\n",
    "    \n",
    "for i in range(10):\n",
    "    generate(rnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
